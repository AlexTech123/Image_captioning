{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1328792,"sourceType":"datasetVersion","datasetId":771078},{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":104008860,"sourceType":"kernelVersion"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms, models\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport nltk\n\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nimport math\n\nfrom PIL import Image\n\nfrom datasets import load_dataset\nimport os\n\nfrom transformers import ViTModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T10:25:17.229807Z","iopub.execute_input":"2024-09-18T10:25:17.230235Z","iopub.status.idle":"2024-09-18T10:25:25.086887Z","shell.execute_reply.started":"2024-09-18T10:25:17.230193Z","shell.execute_reply":"2024-09-18T10:25:25.086061Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:25:25.088414Z","iopub.execute_input":"2024-09-18T10:25:25.089128Z","iopub.status.idle":"2024-09-18T10:25:25.229129Z","shell.execute_reply.started":"2024-09-18T10:25:25.089090Z","shell.execute_reply":"2024-09-18T10:25:25.228060Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data extraction","metadata":{}},{"cell_type":"code","source":"images_path = '/kaggle/input/flickr8kimagescaptions/flickr8k/images'\ncaptions_file = '/kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt'\n\nimgs = []\nprompts = []\n\nwith open(captions_file, 'r') as f:\n    next(f)\n    for i, line in enumerate(tqdm(f)):\n        if i == 40000:\n            break\n        if i % 3 != 0:\n            continue\n        image_name, caption = line.strip().split(',', 1)\n        \n        image_path = os.path.join(images_path, image_name)\n        \n        img = Image.open(image_path).convert('RGB')\n        \n        imgs.append(img)\n        prompts.append(caption)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:25:25.230346Z","iopub.execute_input":"2024-09-18T10:25:25.230661Z","iopub.status.idle":"2024-09-18T10:27:10.634053Z","shell.execute_reply.started":"2024-09-18T10:25:25.230629Z","shell.execute_reply":"2024-09-18T10:27:10.633122Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890755dfb47442c2bf04e3738f6fc8e4"}},"metadata":{}}]},{"cell_type":"code","source":"vocab = set(['<bos>', '<eos>', '<unk>', '<pad>'])\n\nfor prompt in prompts:\n    for word in word_tokenize(prompt):\n        vocab.add(word.lower())","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:10.636747Z","iopub.execute_input":"2024-09-18T10:27:10.637446Z","iopub.status.idle":"2024-09-18T10:27:12.807972Z","shell.execute_reply.started":"2024-09-18T10:27:10.637398Z","shell.execute_reply":"2024-09-18T10:27:12.807174Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.809113Z","iopub.execute_input":"2024-09-18T10:27:12.809486Z","iopub.status.idle":"2024-09-18T10:27:12.816223Z","shell.execute_reply.started":"2024-09-18T10:27:12.809443Z","shell.execute_reply":"2024-09-18T10:27:12.815283Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"5526"},"metadata":{}}]},{"cell_type":"code","source":"word2ind = {word: i for i, word in enumerate(vocab)}\nind2word = {i: word for word, i in word2ind.items()}","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.817358Z","iopub.execute_input":"2024-09-18T10:27:12.817718Z","iopub.status.idle":"2024-09-18T10:27:12.828381Z","shell.execute_reply.started":"2024-09-18T10:27:12.817674Z","shell.execute_reply":"2024-09-18T10:27:12.827582Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_img(img):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    img = transform(img)\n    return img\n\ndef preprocess_prompt(prompt):\n    tokenized_prompt = [word2ind['<bos>']]\n    tokenized_prompt += [word2ind.get(token.lower(), word2ind['<unk>']) for token in word_tokenize(prompt)]\n    tokenized_prompt += [word2ind['<eos>']]\n    \n    return tokenized_prompt","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.829940Z","iopub.execute_input":"2024-09-18T10:27:12.830304Z","iopub.status.idle":"2024-09-18T10:27:12.839195Z","shell.execute_reply.started":"2024-09-18T10:27:12.830264Z","shell.execute_reply":"2024-09-18T10:27:12.838325Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Img2textDataset(Dataset):\n    def __init__(self, imgs, prompts):\n        self.imgs = imgs\n        self.prompts = prompts\n        \n    def __getitem__(self, idx):\n        return preprocess_img(self.imgs[idx]), preprocess_prompt(self.prompts[idx])\n    \n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.840375Z","iopub.execute_input":"2024-09-18T10:27:12.840659Z","iopub.status.idle":"2024-09-18T10:27:12.850115Z","shell.execute_reply.started":"2024-09-18T10:27:12.840628Z","shell.execute_reply":"2024-09-18T10:27:12.849311Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def collate_fn_with_padding(batch):\n    imgs, prompts = zip(*batch)\n    imgs = torch.stack(imgs, dim=0)\n    \n    seq_lens = [len(prompt) for img, prompt in batch]\n    max_seq_len = max(seq_lens)\n    \n    padded_prompts = [prompt + [word2ind['<pad>']] * (max_seq_len - len(prompt)) for prompt in prompts]\n    padded_prompts = torch.LongTensor(padded_prompts)\n    \n    return imgs, padded_prompts[:,:-1], padded_prompts[:,1:]","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.851247Z","iopub.execute_input":"2024-09-18T10:27:12.852010Z","iopub.status.idle":"2024-09-18T10:27:12.862718Z","shell.execute_reply.started":"2024-09-18T10:27:12.851975Z","shell.execute_reply":"2024-09-18T10:27:12.861872Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_imgs, test_imgs, train_prompts, test_prompts = train_test_split(imgs, prompts, test_size=0.2)\n\ntrain_dataset = Img2textDataset(train_imgs, train_prompts)\ntest_dataset = Img2textDataset(test_imgs, test_prompts)\n\nBATCH_SIZE = 64\n\ntrain_dataloader = DataLoader(\n    train_dataset, collate_fn=collate_fn_with_padding, batch_size=BATCH_SIZE, shuffle=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset, collate_fn=collate_fn_with_padding, batch_size=BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.866283Z","iopub.execute_input":"2024-09-18T10:27:12.866546Z","iopub.status.idle":"2024-09-18T10:27:12.882535Z","shell.execute_reply.started":"2024-09-18T10:27:12.866517Z","shell.execute_reply":"2024-09-18T10:27:12.881854Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, embed_dim, num_unfrozen_layers=2):\n        super(Encoder, self).__init__()\n        \n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        \n        for param in self.vit.parameters():\n            param.requires_grad = False\n            \n        if num_unfrozen_layers > 0:\n            for layer in self.vit.encoder.layer[-num_unfrozen_layers:]:\n                for param in layer.parameters():\n                    param.requires_grad = True\n        \n        self.fc = nn.Linear(self.vit.config.hidden_size, embed_dim)\n        \n    def forward(self, x):\n        with torch.no_grad():\n            outputs = self.vit(x)\n            x = outputs.last_hidden_state[:, 0]\n        \n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:00:56.881993Z","iopub.execute_input":"2024-09-18T11:00:56.882367Z","iopub.status.idle":"2024-09-18T11:00:56.891313Z","shell.execute_reply.started":"2024-09-18T11:00:56.882333Z","shell.execute_reply":"2024-09-18T11:00:56.889223Z"},"trusted":true},"execution_count":218,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, hidden_dim, vocab_size, num_layers):\n        super(Decoder, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, dropout=0.5, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, hidden_dim)\n        self.projection = nn.Linear(hidden_dim, vocab_size)\n        self.non_lin = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, input_batch, hidden_state=None):\n        embeddings = self.embedding(input_batch)\n        \n        if hidden_state is None:\n            output, hidden_state = self.rnn(embeddings)\n        else:\n            output, hidden_state = self.rnn(embeddings, hidden_state)\n\n        output = self.dropout(self.linear(self.non_lin(output)))\n        projection = self.projection(output)\n        return projection, hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:00:59.060379Z","iopub.execute_input":"2024-09-18T11:00:59.061095Z","iopub.status.idle":"2024-09-18T11:00:59.069219Z","shell.execute_reply.started":"2024-09-18T11:00:59.061056Z","shell.execute_reply":"2024-09-18T11:00:59.068237Z"},"trusted":true},"execution_count":219,"outputs":[]},{"cell_type":"code","source":"class Img2textModel(nn.Module):\n    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers):\n        super(Img2textModel, self).__init__()\n        \n        self.encoder = Encoder(embed_dim)\n        self.decoder = Decoder(hidden_dim, vocab_size, num_layers)\n        self.init_hidden_layer = nn.Linear(embed_dim, hidden_dim)\n        self.init_cell_layer = nn.Linear(embed_dim, hidden_dim)\n        \n        self.num_layers = num_layers\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)  \n        \n        hidden_state = self.init_hidden_layer(features)\n        cell_state = self.init_cell_layer(features)\n\n        hidden_state = hidden_state.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        cell_state = cell_state.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        \n        outputs, _ = self.decoder(captions, (hidden_state, cell_state))\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:01:01.143092Z","iopub.execute_input":"2024-09-18T11:01:01.143726Z","iopub.status.idle":"2024-09-18T11:01:01.151448Z","shell.execute_reply.started":"2024-09-18T11:01:01.143689Z","shell.execute_reply":"2024-09-18T11:01:01.150533Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"def validate_model(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n\n    with torch.no_grad():\n        for images, inputs, targets in dataloader:\n            images = images.to(device)\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images, inputs)\n            \n            loss = criterion(outputs.view(-1, outputs.size(2)), targets.reshape(-1))\n            \n            running_loss += loss.item()\n\n    validation_loss = running_loss / len(dataloader)\n    print(f'Validation Loss: {validation_loss:.4f}')\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.915283Z","iopub.execute_input":"2024-09-18T10:27:12.915650Z","iopub.status.idle":"2024-09-18T10:27:12.923586Z","shell.execute_reply.started":"2024-09-18T10:27:12.915609Z","shell.execute_reply":"2024-09-18T10:27:12.922657Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs, device):\n    model = model.to(device)\n    model.train()\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        \n        with tqdm(total=len(train_dataloader), desc=\"Training\", unit=\"batch\") as pbar:\n            for i, (images, inputs, targets) in enumerate(train_dataloader):\n                images = images.to(device)\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                outputs = model(images, inputs) # (batch_size, seq_len, vocab_size)\n\n                loss = criterion(outputs.view(-1, outputs.size(2)), targets.reshape(-1))\n\n                optimizer.zero_grad()\n\n                loss.backward()\n\n                optimizer.step()\n\n                running_loss += loss.item()\n\n                if (i + 1) % 10 == 0:\n                    pbar.set_postfix({'Loss': running_loss / (i + 1)})\n    \n                pbar.update(1)\n                \n        epoch_loss = running_loss / len(train_dataloader)\n        print(f'Epoch {epoch + 1} Loss: {epoch_loss:.4f}')\n        \n        validate_model(model, test_dataloader, criterion, device)\n        \n    model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:27:12.924703Z","iopub.execute_input":"2024-09-18T10:27:12.925052Z","iopub.status.idle":"2024-09-18T10:27:12.936691Z","shell.execute_reply.started":"2024-09-18T10:27:12.925003Z","shell.execute_reply":"2024-09-18T10:27:12.935937Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Img2textModel(embed_dim=1024, hidden_dim=512, vocab_size=len(vocab), num_layers=1)\ncriterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:01:06.076371Z","iopub.execute_input":"2024-09-18T11:01:06.077146Z","iopub.status.idle":"2024-09-18T11:01:06.452357Z","shell.execute_reply.started":"2024-09-18T11:01:06.077101Z","shell.execute_reply":"2024-09-18T11:01:06.451324Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"num_params = sum(p.numel() for p in model.parameters())\nparam_size_bytes = 4\ntotal_size_bytes = num_params * param_size_bytes\ntotal_size_megabytes = total_size_bytes / (1024 ** 2)\nprint(f\"Model size: {total_size_megabytes:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:01:08.960668Z","iopub.execute_input":"2024-09-18T11:01:08.961334Z","iopub.status.idle":"2024-09-18T11:01:08.968044Z","shell.execute_reply.started":"2024-09-18T11:01:08.961292Z","shell.execute_reply":"2024-09-18T11:01:08.967134Z"},"trusted":true},"execution_count":222,"outputs":[{"name":"stdout","text":"Model size: 367.18 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train(model, train_dataloader, test_dataloader, criterion, optimizer, 1, device)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:15:39.987861Z","iopub.execute_input":"2024-09-18T11:15:39.988284Z","iopub.status.idle":"2024-09-18T11:17:34.832860Z","shell.execute_reply.started":"2024-09-18T11:15:39.988246Z","shell.execute_reply":"2024-09-18T11:17:34.831889Z"},"trusted":true},"execution_count":224,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/167 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0bd5648d3194407a783fd46bf1ed6c8"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Loss: 2.3674\nValidation Loss: 3.0732\n","output_type":"stream"}]},{"cell_type":"markdown","source":"min_val_loss: 3.06","metadata":{}},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def generate_caption(model, img, word2ind, ind2word, max_length=50, temperature=0.5, device='cpu'):\n    model.to(device)\n    model.eval()\n    \n    img = preprocess_img(img).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        features = model.encoder(img)\n        \n        hidden_state = model.init_hidden_layer(features)\n        cell_state = model.init_cell_layer(features)\n        num_layers = model.num_layers\n        hidden_state = hidden_state.unsqueeze(0).repeat(num_layers, 1, 1)\n        cell_state = cell_state.unsqueeze(0).repeat(num_layers, 1, 1)\n        \n        input_token = torch.LongTensor([word2ind['<bos>']]).unsqueeze(0).to(device)\n        \n        generated_tokens = []\n        \n        for _ in range(max_length):\n            output, (hidden_state, cell_state) = model.decoder(input_token, (hidden_state, cell_state))\n            \n            output = output.squeeze(1) / temperature\n            probabilities = torch.softmax(output, dim=-1)\n            \n            next_token = torch.multinomial(probabilities, 1).item()\n            \n            if next_token == word2ind['<eos>']:\n                break\n            \n            generated_tokens.append(next_token)\n            \n            \n            input_token = torch.LongTensor([next_token]).unsqueeze(0).to(device)\n    \n    generated_caption = ' '.join([ind2word[idx] for idx in generated_tokens if idx in ind2word])\n    model.to('cpu')\n    return generated_caption","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:33:07.724960Z","iopub.execute_input":"2024-09-18T10:33:07.725268Z","iopub.status.idle":"2024-09-18T10:33:07.735436Z","shell.execute_reply.started":"2024-09-18T10:33:07.725236Z","shell.execute_reply":"2024-09-18T10:33:07.734553Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def rebuild_img(img):\n    inv_normalize = transforms.Normalize(\n        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n    )\n\n    img_ = inv_normalize(img)\n\n    img_ = img_.permute(1, 2, 0).numpy()\n\n    img_ = np.clip(img_, 0, 1)\n    \n    return img_","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:33:07.736602Z","iopub.execute_input":"2024-09-18T10:33:07.736927Z","iopub.status.idle":"2024-09-18T10:33:07.748804Z","shell.execute_reply.started":"2024-09-18T10:33:07.736896Z","shell.execute_reply":"2024-09-18T10:33:07.748009Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/kaggle/input/coco-2017-dataset/coco2017/train2017/000000000731.jpg')\nimg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(model, img, word2ind, ind2word, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:21:38.724851Z","iopub.execute_input":"2024-09-18T11:21:38.725197Z","iopub.status.idle":"2024-09-18T11:21:38.983467Z","shell.execute_reply.started":"2024-09-18T11:21:38.725164Z","shell.execute_reply":"2024-09-18T11:21:38.982495Z"},"trusted":true},"execution_count":282,"outputs":[{"execution_count":282,"output_type":"execute_result","data":{"text/plain":"'a man and a woman in a field of grass and a woman in a field .'"},"metadata":{}}]},{"cell_type":"code","source":"for prompt in prompts:\n    if 'front of a crowd of people' in prompt:\n        print(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:44:44.811366Z","iopub.execute_input":"2024-09-18T10:44:44.811748Z","iopub.status.idle":"2024-09-18T10:44:44.818793Z","shell.execute_reply.started":"2024-09-18T10:44:44.811712Z","shell.execute_reply":"2024-09-18T10:44:44.817838Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"A black race car starts up in front of a crowd of people .\nA person wearing a black coat and a helmet with spiky things is posing on the lawn in front of a crowd of people .\n","output_type":"stream"}]}]}